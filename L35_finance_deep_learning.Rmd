```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo    = TRUE,
    message = FALSE,
    warning = FALSE
)
```



# Libraries

```{r}

library(tidyquant)

library(timetk)    
library(lubridate)
library(recipes)
library(yardstick)

# Core
library(tidyverse)
library(janitor)
library(tensorflow)
```

```{r}
tensorflow::install_tensorflow(
     method               = "conda",
     version              = "default",
     envname              = "py3.8",
     conda_python_version = "3.6",
    extra_packages       = c("matplotlib", "numpy", "pandas", "scikit-learn")
)
```


## Python Setup

```{r}
library(reticulate)

use_condaenv("py3.8", required = TRUE)
```


## Special Setup Notes 

### Scikit Learn.

```{python, eval=F}
from sklearn.preprocessing import StandardScaler, Normalizer
```


## Quandl Data

```{r}
gold_spot_prices_tbl <- tidyquant::tq_get(
    "LBMA/GOLD", 
    get  = "quandl", 
    from = "1985-01-01", 
    to   = "2017-01-01", 
    api_key = "") %>%
    clean_names()

gold_spot_prices_tbl 
```

```{r}
#write_rds(gold_spot_prices_tbl, "data/gold_spot_prices.rds")
```


## Visualize the data

```{r}
gold_spot_prices_tbl <- read_rds("data/gold_spot_prices.rds")
```


```{r}
gold_spot_prices_tbl %>%
    plot_time_series(date, usd_pm, .smooth = F, .title = "Gold Spot Prices")
```

## Summarize Monthly

```{r}
gold_monthly_tbl <- gold_spot_prices_tbl %>%
    select(date, usd_pm) %>%
    rename(gold_price = usd_pm) %>%
    summarise_by_time(date, .by = "month", gold_price = mean(gold_price, na.rm = TRUE)) 

gold_monthly_tbl
```

```{r}
gold_monthly_tbl %>%
    plot_time_series(date, gold_price, .smooth = F)
```

```{r}
 write_rds(gold_monthly_tbl, "data/gold_monthly_tbl.rds")
```


# Preprocessing

```{r}
gold_monthly_tbl <- read_rds("data/gold_monthly_tbl.rds")
```


```{r}
train_tbl <- gold_monthly_tbl %>% filter_by_time(date, "start", "2010")
test_tbl  <- gold_monthly_tbl %>% filter_by_time(date, "2011", "2011") 
```


```{r, paged.print = FALSE}
recipe_spec <- recipe(~ ., data = train_tbl) %>%
    step_mutate(pct_change =  (gold_price - lag(gold_price)) / lag(gold_price)) %>%
    step_normalize(contains("pct_change")) %>%
    step_impute_mean(pct_change)

recipe_spec_prep <- recipe_spec %>% prep() 

recipe_spec_prep
```

```{r}
recipe_spec_prep %>% tidy()
```

```{r}
recipe_spec_prep %>% tidy(2)
```


```{r}
train_processed_tbl <- recipe_spec_prep %>% juice()
train_processed_tbl
```

## Return Time Plot

```{r}
train_processed_tbl %>%
    plot_time_series(date, pct_change)
```

## Return Autcorrelation

```{r}
train_processed_tbl %>% plot_acf_diagnostics(date, pct_change)
```

## Return Seasonality

```{r}
train_processed_tbl %>%
    plot_seasonal_diagnostics(date, pct_change)
```

```{r}
# write_rds(train_processed_tbl, "data/train_processed_tbl.rds")
train_processed_tbl <- read_rds("data/train_processed_tbl.rds")

# recipe_spec_prep %>% write_rds("data/recipe_spec_prep.rds")
recipe_spec_prep <- read_rds("data/recipe_spec_prep.rds")
```



# LSTM 

```{python}
import sys
import os
import time
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from datetime import datetime

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, Input
from tensorflow.keras.layers import LSTM

from tensorflow.keras.models import load_model
```

```{python}
tf.random.set_seed(seed=5)
```


## Data Setup

```{python}
df = r.train_processed_tbl
df
```


```{python}
target = np.asarray(df['pct_change'])
target = np.atleast_2d(target)
target = target.T
        
target.shape
```

```{python}
# 12-month lookback period
look_back = 12

X = np.atleast_3d(
    np.array(
        [target[start:start + look_back] for start in range(0, target.shape[0] - look_back)]
    )
)

y = target[look_back:]
```

```{python}
X.shape
```

```{python}
y.shape
```

## LSTM Model

```{python}
model = Sequential()

model.add(Input(shape=(12,1)))
model.add(LSTM(64, return_sequences=True, activation='tanh'))
model.add(LSTM(12, return_sequences=False, activation='relu'))

model.add(Dense(1))

model.add(Activation(activation='linear'))

model.compile(optimizer="rmsprop", loss="mae")

```

```{python}
model.summary()
```


## Model Training (Takes a Minute)

- Epochs: Try 100 vs 1000 - See the differences

```{python, results="hide"}
model.fit(X, y, epochs=1000, batch_size=80, verbose=1, shuffle=False)
```

## Make Predictions (In-Sample)

```{python}
in_sample_predictions = model.predict(X)
in_sample_predictions[0][0]
```

```{python}
in_sample_predictions.shape
```

```{python}
predictions = in_sample_predictions.flatten()
predictions.shape
```



## Residual Analysis


```{r}
train_processed_tbl %>%
    add_column(pred = c(rep(NA, 12), py$predictions)) %>%
    ggplot(aes(pred, pct_change)) +
    geom_abline(slope = 1) +
    geom_point(alpha = 0.8) +
    geom_smooth(method = "lm", se = FALSE)
```





# Forecast (Future Predictions)

## Data Preparation

```{r}
new_data <- train_processed_tbl %>%
    future_frame(.length_out = 12) %>%
    bind_rows(train_processed_tbl, .) %>%
    replace_na(replace = list(pct_change = 0))

new_data %>% tail(14)
```

```{python}
def prepare_data(data, column = 'pct_change', look_back = 12):
    
    target = np.asarray(data[column])
    target = np.atleast_2d(target)

    if target.shape[0] == 1:
        target = target.T
        
    X = np.atleast_3d(
        np.array(
            [target[start:start + look_back] for start in range(0, target.shape[0] - look_back)]
        )
    )
    
    y = target[look_back:]
    
    return X, y
```


```{python}
X_new, y_new = prepare_data(r.new_data)

X_new.shape
```

## Make Prediction

```{python}
def predict_model(model, new_data):
    predictions_2d = model.predict(new_data)
    return predictions_2d.flatten()
```

```{python}
predictions_new = predict_model(model, new_data = X_new)
predictions_new.shape
```

## Residual Analysis

```{r}
forecast_tbl <- recipe_spec_prep %>% 
    bake(new_data = gold_monthly_tbl) %>%
    
    # Change this based on time-window being investigated
    filter_by_time(date, "start", "2011") %>%
    
    mutate(split = ifelse(date %in% train_tbl$date, "Training", "Test")) %>%
    add_column(predictions = c(rep(NA, 12), py$predictions_new))

forecast_tbl
```

```{r}
forecast_tbl %>%
    mutate(split = factor(split, levels = c("Training", "Test"))) %>%
    ggplot(aes(predictions, pct_change, color = split)) +
    geom_point(alpha = 0.8) +
    geom_abline(slope = 1) +
    scale_color_tq() +
    theme_tq()
```


## Visualize Forecast

```{r}
recipe_spec_prep %>% tidy()
```

```{r}
recipe_spec_prep %>% tidy(2)
```

```{r}
standardization_tbl <- recipe_spec_prep %>% tidy(2)
mean <- standardization_tbl %>% filter(statistic == "mean") %>% pull(value) 
sd   <- standardization_tbl %>% filter(statistic == "sd") %>% pull(value)

gold_price_start <- forecast_tbl %>%
    filter(split == "Training") %>%
    slice(n()) %>%
    pull(gold_price)

forecast_prepared_tbl <- forecast_tbl %>%
    filter(split == "Test") %>%
    mutate(predictions     = standardize_inv_vec(predictions, mean = mean, sd = sd)) %>%
    mutate(gold_prediction = gold_price_start * cumprod(1 + predictions)) %>%
    bind_rows(forecast_tbl %>% filter(split == "Training")) %>%
    select(date, gold_price, gold_prediction, split) %>%
    pivot_longer(cols = c(gold_price, gold_prediction)) %>%
    arrange(date, split, name) %>%
    drop_na() 

forecast_prepared_tbl %>%
    plot_time_series(date, value, .color_var = name, .smooth = F, .plotly_slider = T)
    
```



```{r}
forecast_prepared_tbl %>%
    filter(split == "Test") %>%
    pivot_wider(names_from = name, values_from = value) %>%
    group_by(split) %>%
    summarize(
        mae   = mae_vec(gold_price, gold_prediction),
        rmse  = rmse_vec(gold_price, gold_prediction),
        mape  = mape_vec(gold_price, gold_prediction),
        smape = smape_vec(gold_price, gold_prediction)
    ) %>%
    ungroup()
```

# Save Model

```{python}
model.save("models/temp_1.h5")
```

```{python}
temp_1 = load_model("models/temp_1.h5")
```

```{python}
temp_1.summary()
```


## Prepare data for TensorFlow LSTM

`prepare_data()`

```{r}
source_python("py/01_prepare_data.py")

training_data <- new_data %>% drop_na()

data_prepared_list <- prepare_data(training_data, column = "pct_change", look_back = 12L)

glimpse(data_prepared_list)
```

## TF LSTM Model the Prepared Data

`tensorflow_lstm()` 

```{r}
source_python("py/02_tensorflow_model.py")

X = data_prepared_list[[1]]
y = data_prepared_list[[2]]

tensorflow_lstm(X, y, epochs = 10L, activation_1 = "relu", loss = "mae")
```

## Predict New Data

`predict_lstm()`

- Loads model saved as `models/app_model.h5`
- Makes prediction from the TF Model

```{r}
source_python("py/03_model_prediction.py")

new_data_prepared_list <- prepare_data(new_data, "pct_change", look_back = 12L)

predictions <- predict_lstm(new_data_prepared_list[[1]])

glimpse(predictions)
```

```{r}
source_python("py/04_print_model_summary.py")

print_model_summary()
```



``` {python, eval=F}
tf.keras.layers.LSTM(
    units, activation='tanh', recurrent_activation='sigmoid', use_bias=True,
    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',
    bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,
    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,
    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,
    dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False,
    return_state=False, go_backwards=False, stateful=False, time_major=False,
    unroll=False, **kwargs
)
```
